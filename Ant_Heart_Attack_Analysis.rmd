---
title: 'Heart Attack Dataset: An Analysis'
author: "Ananthajit Srikanth"
date: "19/07/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
```

# Introduction

Heart attacks are the leading cause of death worldwide (source: Our World in Data). Hence, understanding how to predict the possibility of heart disease is extremely important. 

This analysis will use the UCI Machine Learning Repository's Heart Disease Dataset to predict the risk of heart disease in patients. It will use a variety of classification methods (such as discriminant analysis and )

## Dataset Information

This analysis uses four datasets, each of which are from four different regions:

* Hungarian Institute of Cardiology
* University Hospital, Zurich, Switzerland and University Hospital, Basel, Switzerland
* Cleveland Clinic Foundation
* V.A. Medical Center, Long Beach

Each of these regions form the *processed.hungarian*,*processed.switzerland*,*processed.cleveland*, and *processed.va* datasets.

The *processed* prefix indicates that out of 76 variables, only 14 are used. All the tables are also comma separated.

Given below are the variables in the dataset, as per the heart-disease.names file:

1. Age in years
2. Sex (assigned at birth) (1 is male, and 0 is female)
3. Chest Pain Type (one of four values):
  1 - Typical Angina
  2 - Atypical Angina
  3 - Non anginal Pain
  4 - Asymptomatic
4. trestbps - Resting Blood Pressure in mm Hg
5. chol - Serum Cholesterol in mg/dl
6. fbs - Is fasting blood sugar greater than 120 mg/dl? (1 = yes, 0 = no)
7. restecg - Resting Electrocardiogram (ECG) results (one of three values):
  0 - Normal
  1 - Having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV): This is a symptom of Myocardial Infarction
  2 - Showing probable or definite left ventricular hypertrophy by Estes' criteria (enlargment of the left ventricle, usually to compensate for tissue loss)
8. thalach - Maximum Heart Rate Achieved (in beats per minute)
9. exang - Exercise Induced Angina (1 = yes, 0 = no)
10. oldpeak - ST depression induced by exercise relative to rest
11. slope - Slope of ST segment (as seen in ECG) during excercise:
  * 1 - Upsloping
  * 2 - Flat
  * 3 - Downsloping
12. ca - Number of vessels colored by fluoroscopy (0 - 3) 
13. thal: One of three possible values:
  * 3 - Normal
  * 6 - Fixed Defect
  * 7 - Reversible Defect

14. num (outcome) - Diagnosis of Heart Disease:
  1 - More than 50% narrowing of diameter of any major heart vessel
  0 - Less than 50% narrowing of diameter of any major heart vessel.  
  2,3, and 4 also indicate heart disease, of varying severity.
## Creating the Datasets

The datasets are publicly available at the [UCI Machine Learning repository website](https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/) 

First, libraries have to be loaded for analysis:

```{r}
# Heart attack analysis

# Loading the required libraries, can add required libraries later in vector (same code as used in previous project, source: Ananthajit Srikanth)
required_libraries <- c("caret", "matrixStats", "tidyverse", "knitr", "broom", "ranger", "knitr", "rmarkdown", "mice")
# The for loop installs and loads libraries one by one from required_libraries
for(i in 1:length(required_libraries)){
  if(!require(required_libraries[i], character.only = TRUE)){
    install.packages(required_libraries[i], repos = "http://cran.us.r-project.org")
    library(required_libraries[i], character.only = TRUE)
  }
  else{
    require(required_libraries[i], character.only = TRUE)}}

# Run this line only if necessary:
# update.packages()
```

Once the libraries are loaded, the datasets can be downloaded:
Looking at one file, we can see the structure of the data:
```{r}
# Show that the files are comma-separated
download.file(url = "https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data", destfile = 'processed.cleveland.data')
readLines(con = 'processed.cleveland.data', n = 1)

```
We can see that the file has comma-separated values.

Looking at the other three tables:

```{r}
download.file(url = "https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.hungarian.data", destfile = "processed.hungarian.data")

download.file(url = "https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.va.data", destfile = "processed.va.data")

download.file(url = "https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.switzerland.data", destfile = "processed.switzerland.data")

noquote("Hungarian")
readLines(con = 'processed.hungarian.data', n = 1)
noquote("Switzerland")
readLines(con = 'processed.switzerland.data', n = 1)
noquote("va")
readLines(con = 'processed.va.data', n = 1)

```

They are also comma-separated. It can also be seen that some fields have a ?. This can be replaced by an NA.

The files can now be downloaded in the csv format:

```{r}
# Downloading the processed.data files as csv

download.file(url = "https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data", destfile = "cleveland.csv")

download.file(url = "https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.hungarian.data", destfile = "hungarian.csv")

download.file(url = "https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.va.data", destfile = "va.csv")

download.file(url = "https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.switzerland.data", destfile = "switzerland.csv")


```

Based on the documentation provided in heart-disease.names in the data folder, the column names can be stored in a vector:


```{r}
# Column names for data frames
colnames_heartattack <- c("age", "sex", "cp", "trestbps", "chol", "fbs", "restecg", "thalach", "exang", "oldpeak", "slope", "ca", "thal", "num")
```

As the csv files have been downloaded, now they can be read.

```{r}
cleveland <- read_csv("cleveland.csv", col_names = colnames_heartattack)


```

As can be seen, read_csv converted ca and thal (which use numbers as categories) as characters, most likely due to the presence of '?'

```{r}
any(str_detect(cleveland$thal, pattern = "\\?"))
```
This shows that there is at least one "?" in the column *thal*, hence it has been assigned the character variable type.

As all of the columns contain numbers/numerical values, `as.numeric()` can be used to convert all the columns into numerics. However, the ?s have to be converted to NAs first.

```{r}
# Convert all ? to NA
cleveland[cleveland == "?"] <- NA
# Convert all columns to numeric
cleveland <- apply(cleveland, 2, function(x) as.numeric(x)) %>% as.data.frame()

```

As there are multiple datasets, the region of the dataset also has to be specified. In the case of the *cleveland* dataset, the variable *dataset* shall be set to *cleveland*

```{r}
# specify the dataset region
cleveland <- cleveland %>% mutate(dataset = 'cleveland')

```


The same process is repeated for the other three datasets:

```{r}
invisible({
switzerland <- read_csv("switzerland.csv", col_names = colnames_heartattack)
switzerland[switzerland == "?"] <- NA
switzerland <- apply(switzerland, 2, function(x) as.numeric(x)) %>% as.data.frame()
switzerland <- switzerland %>% mutate(dataset = 'switzerland')

hungarian <- read_csv("hungarian.csv", col_names = colnames_heartattack)
hungarian[hungarian == "?"] <- NA
hungarian <- apply(hungarian, 2, function(x) as.numeric(x)) %>% as.data.frame()
hungarian <- hungarian %>% mutate(dataset = "hungarian")

va <- read_csv("va.csv", col_names = colnames_heartattack)
va[va == "?"] <- NA
va <- apply(va, 2, function(x) as.numeric(x)) %>% as.data.frame()
va <- va %>% mutate(dataset = "va")
})

```

The datasets can now be joined together in a table called *total_set*:

```{r}
# join the four datasets together
total_set <- bind_rows(cleveland, switzerland, hungarian, va)

```

Before perfoming any further data cleaning, further analysis has to be done.

```{r}
for(i in 1:(ncol(total_set)-1)) {
 x <- total_set[,i]
 x <- data.frame(x = x)
 name <- names(total_set)[i]
  print(x %>% ggplot(aes(x)) + geom_histogram() + xlab(label = name))
 }

```



Given above is a histogram for all the variables. Most of the variables seems to show reasonable distibutions, except the *chol* variable, which has a skewed distribution. It seems quite unreasonable for anyone to have 0 mg/dl cholesterol.

```{r}
# check error cholesterol

total_set %>% group_by(dataset) %>% summarize(zeros = sum(chol == 0, na.rm = TRUE)) %>% knitr::kable()

```

This shows that the data was probably not input correctly. So, all zeros will be converted to NAs:

```{r}

mutated_chol <- total_set$chol
mutated_chol[mutated_chol == 0] <- NA
total_set <- total_set %>% mutate(chol = mutated_chol)



```

From the dataset documentation, although all columns use numerical values, some numerical values are used to represent categorical outcomes, so these have to be converted to factors:


```{r}
# convert factors
total_set <- total_set %>% mutate(num = as.factor(num), sex = as.factor(sex), cp = as.factor(cp), fbs = as.factor(fbs), restecg = as.factor(restecg),exang = as.factor(exang), slope = as.factor(slope), ca = as.factor(ca), thal = as.factor(thal), dataset=as.factor(dataset))
```

We can now look at the structure of *total_set*

```{r}
str(total_set)

```

While most of the documentation claims that the num variable is binary, looking at the structure of the dataset, there is also factor levels 2, 3 and 4 for the num variable. However, as these imply increasing levels of severity of heart disease, these can also be included with factor level 1. This coverts this problem into a binary classification problem.


```{r}
# Converts 2,3 and 4 into 1.
# as.character has to be used since as.numeric converts factor into level index, for example 0 becomes 1, 1 becomes 2, etc.
temp_num <- as.numeric(as.character(total_set$num))

temp_num_2 <- case_when(temp_num == 0 ~ 0,
                        temp_num %in% c(1,2,3,4) ~ 1,
                        TRUE ~ NA_real_
                      )
remove(temp_num)

total_set <- total_set %>% mutate(num = factor(temp_num_2))
str(total_set$num)
total_set$num %>% head()


```


```{r}
table(total_set$num)
```

We can also see that the prevalence of both 0 and 1 is very close to one another.


Now, the dataset can be checked for NAs.

```{r}
apply(total_set,2,function(x)sum(is.na(x))) %>% knitr::kable()
```
Many columns have NAs. This has to be addressed before training as multiple models do not support datasets with NAs.

```{r}
nas <- apply(total_set, 1, function(x)sum(is.na(x)))
hist(nas, col = '#8898f2')
```
The majority of data points have 0 - 3 NA values, so the mice library can be used to impute values.

Before addressing the NAs, the datasets have to be split into three parts, a training set, a 'debug set' to test models on, and a final hold-out test set.

# Splitting the datasets

From looking at the structure of *total_set* there are 920 rows, so having approx. 125 debugging and 125 testing examples should be adequate.

```{r}
# set.seed for consistency
set.seed(2000, sample.kind = 'Rounding') # remove sample.kind argument if you use R 3.5

split_index <- createDataPartition(y = total_set$num, times = 1, p = 250/960, list = FALSE)

train_set <- total_set[-split_index,]
temp_set <- total_set[split_index,]

set.seed(9000, sample.kind = 'Rounding')

# split debug and test into approx 125 examples each
split_index_2 <- createDataPartition(y = temp_set$num, times = 1, p = 0.5, list = FALSE)

debug_set <- temp_set[-split_index_2,]
test_set <- temp_set[split_index_2,]

remove(temp_set, split_index, split_index_2)

```

We can now look at the structure of the debug set:


```{r}
# debug set structure

str(debug_set)
```

and the test set:

```{r}
# test set structure
str(test_set)
```
The test and debug sets have approximately 120 entries each.


We can begin the analysis. However, multiple training algorithms require there to not be any NAs. So, the data must be imputed.

# Imputing values

Imputation is a method by which values that will be used for training are predicted, based on existing data. For example, if the ca variable is an NA, but restecg is 1 (heart disease risk) and chol (cholesterol) is high, then it would be illogical to predict 3 (3 arteries with bloodflow and less blockage in fluoroscopy), and an imputation method would predict, for example, a 1.

Imputation is only done after splitting the dataset. If performed before splitting, values in the debug/test set may be imputed based on predictions made not only using the debug/test set, but also using the training set, which may lead to skewed imputes, since there may be differences in the datasets after splitting. (Imputation relies on relations between variables, which can vary between datasets.) Imputation is technically part of the training/testing process, since it modifies data values, so when debugging/testing, 

```{r, echo = FALSE}

# impute (insert by prediction) values using mice
# rf works with all data types and is fast and efficient


# remove outcome column so that imputation does not rely on outcome:

train_set_no_outcomes <- train_set %>% select(-num)
# run mice
invisible({
mice_imputation <- mice(train_set_no_outcomes, blocks = c("trestbps", "chol", "fbs", "restecg", "thalach", "exang", "oldpeak","slope", "ca", "thal"), method = 'rf', maxit = 50, m = 10)})
train_set_2 <- complete(mice_imputation)

# First entries don't have NAs
# identical(head(test), head(total_set_no_outcomes)) is TRUE so order is maintained
# sum(is.na(train_set_2)) is 0

train_set_2 <- train_set_2 %>% mutate(num = train_set$num)

# repeat for debug and test set
debug_set_no_outcomes <- debug_set %>% select(-num)
# run mice
invisible({
mice_imputation_debug <- mice(debug_set_no_outcomes, blocks = c("trestbps", "chol", "fbs", "restecg", "thalach", "exang", "oldpeak","slope", "ca", "thal"), method = 'rf', maxit = 50, m = 10)})
debug_set_2 <- complete(mice_imputation_debug)
debug_set_2 <- debug_set_2 %>% mutate(num = debug_set$num)

test_set_no_outcomes <- test_set %>% select(-num)
# run mice
invisible({
mice_imputation_test <- mice(test_set_no_outcomes, blocks = c("trestbps", "chol", "fbs", "restecg", "thalach", "exang", "oldpeak","slope", "ca", "thal"), method = 'rf', maxit = 50, m = 10)})
test_set_2 <- complete(mice_imputation_test)
test_set_2 <- test_set_2 %>% mutate(num = test_set$num)


```


Based on the structure of these values, classification algorithms can now be created. But first, an error/accuracy metric has to be defined.

## The accuracy metric.

Since this is a binary classification problem, an accuracy metric has to be defined. In this case, the F1 score can be used. F1 is defined as follows:

$$F_1 = 2\cdot\frac{precision \cdot recall}{precision + recall}$$
Where precision is the ratio of true positives to the number of terms predicted positive. (In this case, positive means that the person has heart disease).
Recall is the proportion of true positive to the number of terms that are actually positive (source: [https://rafalab.github.io/dsbook/introduction-to-machine-learning.html](https://rafalab.github.io/dsbook/introduction-to-machine-learning.html))
Precision and recall are also known as specificity and sensitivity respectively. A high specificity implies a higher number of true positives to false positives. False positives are dangerous as it may lead individuals to seek treatment they do not need. This is not only expensive, but also potentially harmful.
A high sensitivity implies that there are more true positives than false negatives. False negatives are also dangerous since it may lead people at risk of heart disease, to __not__ pursue treatment. This is also dangerous.

Hence, sensitivity and specificity are equally important. So, the F_1 score is not adjusted for either one.

The F1 score can be calculated using the F_meas function in the caret library.

Now, we can proceed with the first models.


# Analysis

Since the datasets are very small, the caret library can be used for classification.
The available models are [here](https://topepo.github.io/caret/available-models.html).

This analysis will use classification models, since regression will not work on factor variables.

The first model to be trained on is a logistic regression model (as a baseline)

```{r}
# logistic model because why not

fit_glm <- train(num~., data=train_set_2, method = 'glm')

# see result on debug set

predict_glm <- predict(fit_glm, newdata = debug_set_2)

# levels(debug_set_2$num)[2] is '1'
fmeas1 <- F_meas(data = predict_glm, reference = debug_set_2$num, relevant = levels(debug_set_2$num)[2])
fmeas1
```

This is a very good result! However, can more be done?

The next models that can be tried out are discriminant analyses models.
First is LDA:

```{r}
# lda model
fit_lda <- train(num~., data=train_set_2, method = 'lda')

# see result on debug set

predict_lda <- predict(fit_lda, newdata = debug_set_2)

# levels(debug_set_2$num)[2] is '1'
fmeas2 <- F_meas(data = predict_lda, reference = debug_set_2$num, relevant = levels(debug_set_2$num)[2])
fmeas2
```
A slight improvement is observed.
Now, qda can be tested:


```{r}
# qda model
fit_qda <- train(num~., data=train_set_2, method = 'qda')

# see result on debug set

predict_qda <- predict(fit_qda, newdata = debug_set_2)

# levels(debug_set_2$num)[2] is '1'
fmeas3 <- F_meas(data = predict_qda, reference = debug_set_2$num, relevant = levels(debug_set_2$num)[2])
fmeas3


```

We see that there is a drop in F1. This suggests that qda is not an effective classification model.

The next model that can be tested is Naive Bayes.

```{r}

# naive bayes model
fit_nb <- train(num~., data=train_set_2, method = 'naive_bayes')

# see result on debug set

predict_nb <- predict(fit_nb, newdata = debug_set_2)

# levels(debug_set_2$num)[2] is '1'
fmeas4 <- F_meas(data = predict_nb, reference = debug_set_2$num, relevant = levels(debug_set_2$num)[2])
fmeas4


```

A considerable improvement from qda, albeit worse than logistic regression or lda.


The next models that can be considered are classification trees

The first method that can be tested is classification trees using CART, and the rpart library.

```{r}
# classification trees model
fit_cart <- train(num~., data=train_set_2, method = 'rpart')

# see result on debug set

predict_cart <- predict(fit_cart, newdata = debug_set_2)

# levels(debug_set_2$num)[2] is '1'
fmeas5 <- F_meas(data = predict_cart, reference = debug_set_2$num, relevant = levels(debug_set_2$num)[2])
fmeas5



# do nnet later
```




















# Bibliography
1. Hungarian Institute of Cardiology. Budapest: Andras Janosi, M.D.
       2. University Hospital, Zurich, Switzerland: William Steinbrunn, M.D.
       3. University Hospital, Basel, Switzerland: Matthias Pfisterer, M.D.
       4. V.A. Medical Center, Long Beach and Cleveland Clinic Foundation:
	  Robert Detrano, M.D., Ph.D.
https://ourworldindata.org/causes-of-death
https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/

https://rafalab.github.io/dsbook/introduction-to-machine-learning.html

https://topepo.github.io/caret/available-models.html